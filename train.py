# train.py

import os
import time
import pythoncom

from agent.dqn_agent import DQNAgent
from env.simulation_env import SimulationEnvironment
from buffer.pending_buffer import PendingBuffer
from utils.order_generator import OrderGenerator
from interface.plantsim_interface import PlantsimInterface
from utils.logger import Logger
from config import DEFAULT_HYPERPARAMS, DEFAULT_SIM_PARAMS


def run_training(params, log_callback=None, csv_path=None):
    """
    í•™ìŠµ ì‹¤í–‰ í•¨ìˆ˜
    :param params: dict í˜•íƒœì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    :param log_callback: ë¡œê·¸ ì¶œë ¥ í•¨ìˆ˜ (GUIìš©)
    """
    pythoncom.CoInitialize()  # COM ê°ì²´ ì´ˆê¸°í™” (GUI ì“°ë ˆë“œì—ì„œ í•„ìˆ˜)
    
    # ê¸°ë³¸ê°’ ì´ˆê¸°í™”
    full_params = {**DEFAULT_HYPERPARAMS, **DEFAULT_SIM_PARAMS}
    if params:
        full_params.update(params)

    logger = Logger(gui_callback=log_callback)

    # ì¸í„°í˜ì´ìŠ¤ ë° í™˜ê²½ ì´ˆê¸°í™”
    plsim = PlantsimInterface()
    model_file = "tp_v11.spp"
    model_path = os.path.abspath(model_file)
    plsim.initialize_model(model_path)

    env = SimulationEnvironment(plsim)
    state_dim = env.get_state_dim()
    action_dim = env.get_action_dim()

    # DQN ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
    agent = DQNAgent(
        state_dim=state_dim,
        action_dim=action_dim,
        gamma=full_params["Gamma"],
        epsilon=full_params["Epsilon"],
        lr=full_params["LearningRate"],
        batch_size=full_params["BatchSize"]
    )

    order_gen = OrderGenerator(csv_path=csv_path) if csv_path else OrderGenerator()
    buffer = PendingBuffer()

    num_episodes = full_params["Episode"]
    T = full_params["OrderInterval"]

    current_order = order_gen.generate_order()
    next_order = order_gen.generate_order()

    for episode in range(num_episodes):
        logger.log_text(f"[Episode {episode + 1}]")

        env.reset_and_initialize(T)
        buffer.clear()
        order_gen.reset()

        while not env.is_terminal() and order_gen.has_next():
            env.register_order(current_order)

            if not env.has_idle_ams():
                env.start_simulation()
                while not env.has_idle_ams() and not env.is_terminal():
                    time.sleep(1)
                env.stop_simulation()

            if env.has_idle_ams():
                state = env.get_state(current_order)
                next_state = env.get_state(next_order)

                action = agent.select_action(state)
                ams_index = action + 1
                env.assign_order(ams_index)

                buffer.add(current_order["order_id"], state, action, next_state)

            env.run_simulation_for_T(T)

            for order_id, reward in env.get_completed_rewards(buffer.keys()):
                item = buffer.pop(order_id)
                if item:
                    agent.remember(item["state"], item["action"], reward, item["next_state"])
                    agent.update()
                    logger.log(episode + 1, order_id, reward)

            current_order = next_order
            if order_gen.has_next():
                next_order = order_gen.generate_order()
            else:
                break  # ë‹¤ìŒ ì˜¤ë”ê°€ ì—†ìœ¼ë©´ ë£¨í”„ ì¢…ë£Œ

        agent.update_target()
        logger.log_text(f"  Episode {episode + 1} completed\n")

    # ê²°ê³¼ ì €ì¥
    model_path = os.path.join(logger.get_save_dir(), "model.pth")
    agent.save_model(model_path)
    logger.save_graph()
    logger.log_text(f"ğŸ“ ê²°ê³¼ ì €ì¥ ì™„ë£Œ â†’ {logger.get_save_dir()}")

    # ì‹œë®¬ë ˆì´í„° ì¢…ë£Œ
    plsim.quit()
